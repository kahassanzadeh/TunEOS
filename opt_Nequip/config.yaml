# The config file is divided into 4 sections -- `data`, `train`, `model`, and `global_options`
# The config system relies on omegaconf (https://omegaconf.readthedocs.io/en/2.3_branch/index.html)
# and hydra (https://hydra.cc/docs/intro/) functionalities, such as
# - omegaconf's variable interpolation (https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#variable-interpolation)
# - omegaconf's resolvers (https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#resolvers)
# - hydra's instantiate (https://hydra.cc/docs/advanced/instantiate_objects/overview/)
# With hydra's instantiation (notice the "_target_"s everywhere), the config file (almost) directly corresponds
# to instantiating objects as one would normally do in Python.
# Much of the infrastructure is based on PyTorch Lightning (https://lightning.ai/docs/pytorch/stable/),
# such as the use of Lightning's Trainer, DataModule, LightningModule, Callback objects.


# the run types will be completed in sequence
# one can do `train`, `val`, `test`, `predict` run types
run: [train, test]


# the following parameters (cutoff_radius, chemical_symbols, model_type_names) are not used direcly by the code
# parameters that take thier values show up multiple times in the config, so this allows us to use
# variable interpolation to keep their multiple instances consistent

# data and model r_max can be different (model's r_max should be smaller), but we try to make them the same
cutoff_radius: 6.0

# There are two sets of atomic types to keep track of in most applications
# -- there is the conventional atomic species (e.g. C, H), and a separate `type_names` known to the model.
# The model only knows types based on a set of zero-based indices and user-given `type_names` argument.
# An example where this distinction is necessary include datasets with the same atomic species with different charge states:
# we could define `chemical_symbols: [C, C]` and model `type_names: [C3, C4]` for +3 and +4 charge states.
# There could also be instances such as coarse graining we only care about the model's `type_names` (no need to define chemical species).
# Because of this distinction, these variables show up as arguments across different categories, including, data, model, metrics and even callbacks.
# In this case, we fix both to be the same, so we define a single set of each here and use variable interpolation to retrieve them below.
# This ensures a single location where the values are set to reduce the chances of mis-configuring runs.
chemical_symbols: [W]
model_type_names: ${chemical_symbols}


# data is managed by LightningDataModules
# nequip provides some standard datamodules that can be found in nequip.data.datamodule
# users are free to define and use their own datamodules that subclass nequip.data.datamodule.NequIPDataModule
data:
  _target_: nequip.data.datamodule.ASEDataModule
  seed: 456             # dataset seed for reproducibility

  # here we take an ASE-readable file (in extxyz format) and split it into train:val:test = 80:10:10
  split_dataset:
    file_path: /home/hassanza/final_test/TunEOS/opt_Nequip/filtered_structures.xyz
    train: 0.85
    val: 0.1
    test: 0.05

  # `transforms` convert data from the Dataset to a form that can be used by the ML model
  # the transforms are only performed right before data is given to the model
  # data is kept in its untransformed form

  transforms:
    # data doesn't usually come with a neighborlist -- this tranforms prepares the neighborlist
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}
    # the models only know atom types, which can be different from the chemical species (e.g. C, H)
    # for instance we can have data with different charge states of carbon, which means they are
    # all labeled by chemical species `C`, but may have different atom type labels based on the charge states
    # in this case, the atom types are the same as the chemical species, but we still have to include this
    # transformation to ensure that the data has 0-indexed atom type lists used in the various model operations
    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}

  # the following are torch.utils.data.dataloader arguments except for `dataset` and `collate_fn`
  # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader
  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 8
    num_workers: 5
    shuffle: true
  val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 8
    num_workers: ${data.train_dataloader.num_workers}  # we want to use the same num_workers -- variable interpolation helps
  test_dataloader: ${data.val_dataloader}  # variable interpolation comes in handy again

  # dataset statistics can be calculated to be used for model initialization such as for shifting, scaling and standardizing.
  # it is advised to provide custom names -- you will have to retrieve them later under model to initialize certain parameters to the dataset statistics computed
  # stats_manager:
  #   # dataset statistics is handled by the DataStatisticsManager
  #   _target_: nequip.data.DataStatisticsManager
  #   # dataloader kwargs for data statistics computation
  #   # `batch_size` should ideally be as large as possible without trigerring OOM
  #   dataloader_kwargs:
  #     batch_size: 8
  #   metrics:
  #     - field:
  #         _target_: nequip.data.NumNeighbors
  #       metric:
  #         _target_: nequip.data.Mean
  #       name: num_neighbors_mean
  #     - field:
  #         _target_: nequip.data.PerAtomModifier
  #         field: total_energy
  #       metric:
  #         _target_: nequip.data.Mean
  #       name: per_atom_energy_mean
  #     # we can also compute per_type statistics
  #     - field: forces
  #       metric:
  #         _target_: nequip.data.RootMeanSquare
  #       per_type: true
  #       name: per_type_forces_rms
  #     # or compute the regular ones
  #     #- field: forces
  #     #  metric:
  #     #    _target_: nequip.data.RootMeanSquare
  #     #  name: forces_rms
  stats_manager:
    # dataset statistics is handled by the `DataStatisticsManager`
    # here, we use `CommonDataStatisticsManager` for a basic set of dataset statistics for general use cases
    # the dataset statistics include `num_neighbors_mean`, `per_atom_energy_mean`, `forces_rms`, `per_type_forces_rms`
    _target_: nequip.data.CommonDataStatisticsManager
    # dataloader kwargs for data statistics computation
    # `batch_size` should ideally be as large as possible without trigerring OOM
    dataloader_kwargs:
      batch_size: 8
    # we need to provide the same type names that correspond to the model's `type_names`
    # so we interpolate the "central source of truth" model type names from above
    type_names: ${model_type_names}

# `trainer` (mandatory) is a Lightning.Trainer object (https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-class-api)
trainer:
  _target_: lightning.Trainer
  accelerator: gpu
  precision: 16-mixed
  devices: 'auto'
  strategy: ddp
  num_nodes: 1
  enable_checkpointing: true
  max_epochs: 20
  max_time: 00:10:00:00
  check_val_every_n_epoch: 1  # how often to validate
  log_every_n_steps: 1       # how often to log


  # use any Lightning supported logger
  logger:
    # Lightning wandb logger https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.wandb.html#module-lightning.pytorch.loggers.wandb
    _target_: lightning.pytorch.loggers.wandb.WandbLogger
    project: test_integration
    name: W229HeH_rmin0.5_rmax6_256-128-0-0-2
    save_dir: ./output
    reinit: true

  # use any Lightning callbacks https://lightning.ai/docs/pytorch/stable/api_references.html#callbacks
  # and any custom callbakcs that subclass Lightning's Callback parent class
  callbacks:
    # Common callbacks used in ML

    # stop training when some criterion is met
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      min_delta: 1e-3                       # how much to be considered a "change"
      patience: 50                            # how many instances of "no change" before stopping

    # checkpoint based on some criterion
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      dirpath: ./output
      filename: best                          # best.ckpt is the checkpoint name
      save_last: true                         # last.ckpt will be saved

    # log learning rate, e.g. to monitor what the learning rate scheduler is doing
    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: epoch

    # use EMA for smoother validation curves and thus more reliable metrics for monitoring
    # - _target_: nequip.train.callbacks.NeMoExponentialMovingAverage
    #   decay: 0.99
    #   every_n_steps: 1

    # or use Lightning's SWA
    #- _target_: lightning.pytorch.callbacks.StochasticWeightAveraging
    #  swa_lrs: 1e-4
    #  swa_epoch_start: 50
    #  annealing_epochs: 20

    # Callbacks to handle loss coefficients to balance different objectives (energy, forces, etc)

    # SoftAdapt scheme (https://arxiv.org/abs/2403.18122) to adaptively change loss coefficients
    #- _target_: nequip.train.callbacks.SoftAdapt
    #  beta: 1.1         # controls strength of SoftAdapt loss coefficient updates
    #  interval: epoch   # update on "epoch" or "batch" basis
    #  frequency: 5      # number of intervals (epoch or batches) between SoftAdapt loss coefficient updates

    # or manually schedule changing of loss coefficients at the start of each training epoch
    #- _target_: nequip.train.callbacks.LossCoefficientScheduler
    #  schedule:
    #    - epoch: 2
    #      coeffs: [3, 1]
    #    - epoch: 5
    #      coeffs: [10, 1]

    # to log the loss coefficients
    - _target_: nequip.train.callbacks.LossCoefficientMonitor
      interval: epoch
      frequency: 5

# training_module refers to a NequIPLightningModule
training_module:
  _target_: nequip.train.EMALightningModule
  ema_decay: 0.999

  # use a MetricsManager (see docs) to construct the loss function
  loss:
    _target_: nequip.train.EnergyForceStressLoss  #MSE
    per_atom_energy: true
    coeffs:
      total_energy: 1.0
      forces: 1.0
      stress: 1.0

  # again, we use a simplified MetricsManager wrapper (see docs) to construct the energy-force metrics
  # the more general `nequip.train.MetricsManager` could also be used in this case
  # validation metrics are used for monitoring and influencing training, e.g. with LR schedulers or early stopping, etc
  val_metrics:
    _target_: nequip.train.EnergyForceStressMetrics
    coeffs:
      total_energy_rmse: 1.0  # to test: per_atom_energy_rmse: 1.0
      forces_rmse: 1.0
      stress_rmse: 1.0

  # use a MetricsManager (see docs) to construct the metrics used for monitoring
  # and influencing training, e.g. with LR schedulers or early stopping, etc
  # we could have train_metrics and test_metrics be different from val_metrics, but it makes sense to have them be the same
  train_metrics: ${training_module.val_metrics}  # use variable interpolation
  test_metrics: ${training_module.val_metrics}  # use variable interpolation

  # any torch compatible optimizer: https://pytorch.org/docs/stable/optim.html#algorithms
  optimizer:
    _target_: torch.optim.Adam
    lr: Tuneos(0.001, 0.01, 0.05)

  # see options for lr_scheduler_config
  # https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule.configure_optimizers
  lr_scheduler:
    # any torch compatible lr sceduler
    scheduler:
      _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
      factor: 0.8
      patience: 20
      threshold: 1e-4
      min_lr: 1e-6
    monitor: val0_epoch/weighted_sum
    interval: epoch
    frequency: 1

  # model details
  model:
    _target_: nequip.model.NequIPGNNModel

    # == basic model params ==
    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    # == bessel encoding ==
    num_bessels: 8               # number of basis functions used in the radial Bessel basis, the default of 8 usually works well
    bessel_trainable: false        # set true to train the bessel weights (default false)
    polynomial_cutoff_p: 6        # p-exponent used in polynomial cutoff function, smaller p corresponds to stronger decay with distance

    # == convnet layers and radial network ==
    num_layers: 1       # number of interaction blocks, we find 3-5 to work best
    l_max: 1           # the maximum irrep order (rotation order) for the network's features, l=1 is a good default, l=2 is more accurate but slower
    parity: true        # whether to include features with odd mirror parity; often turning parity off gives equally good results but faster networks, so do consider this
    num_features: 32    # the multiplicity of the features, 32 is a good default for accurate network, if you want to be more accurate, go larger, if you want to be faster, go lower

    # chemical_embedding_irreps_out: 256x0e
    # irreps_edge_sh: 0e + 1o + 2e + 3o + 4e

    # radial_mlp_depth: [1, 1, 1]          # number of radial layers, usually 1-3 works best, smaller is faster
    # radial_mlp_width: [256, 256, 256]  # number of hidden neurons in radial function, smaller is faster

    # feature_irreps_hidden:
    #   - "256x0o+256x0e+128x1o+128x1e+2x4o+2x4e"
    #   - "256x0o+256x0e+128x1o+128x1e+2x4o+2x4e"
    #   - "256x0o+256x0e+128x1o+128x1e+2x4o+2x4e"

    # conv_to_output_hidden_irreps_out: 128x0e

    # dataset statistics used to inform the model's initial parameters for normalization, shifting and rescaling
    # we use omegaconf's resolvers (https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#resolvers)
    # to facilitate getting the dataset statistics from the DataStatisticsManager

    # average number of neighbors for edge sum normalization
    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}

    # == per-type per-atom scales and shifts ==
    per_type_energy_shifts: [-2052.022] #${training_data_stats:per_atom_energy_mean}
    per_type_energy_shifts_trainable: false
    per_type_energy_scales: ${training_data_stats:per_type_forces_rms}
    per_type_energy_scales_trainable: true

    # == ZBL pair potential ==
    pair_potential:
      _target_: nequip.nn.pair_potential.ZBL
      units: metal     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
      chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types

# global options
global_options:
  allow_tf32: false

